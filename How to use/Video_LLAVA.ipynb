{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8p18twI8dTm"
      },
      "source": [
        "üåü **Exquisitely Crafted By - [OEvortex](https://www.youtube.com/channel/@OEvortex)**\n",
        "\n",
        "üöÄ Have questions or feedback about this video? Reach out to OEvortex via the social links below. Your insights and queries are greatly appreciated!\n",
        "\n",
        "- **YouTube Channel**: [@OEvortex](https://www.youtube.com/@OEvortex)\n",
        "- **Telegram Group**: [Telegram](https://t.me/vortexcodebase)\n",
        "- **Discord Server**: [Join the Community on Discord](https://discord.gg/DugWefkN5Z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3MR2-TC8aIp"
      },
      "source": [
        "----------------------------------------------------------------------\n",
        "üåü **The Video-LLaVA model** is an open-source multimodal model fine-tuned from LLM (Large Language Models) on multimodal instruction-following data. It falls under the auto-regressive language model category and is built on the transformer architecture. The base LLM used for fine-tuning is lmsys/vicuna-13b-v1.5.\n",
        "\n",
        "**Model Description:**\n",
        "- The Video-LLaVA model's key feature is its ability to generate interleaving images and videos, even when image-video pairs are absent in the dataset.\n",
        "- This is achieved by leveraging an encoder trained for unified visual representation through alignment before projection.\n",
        "- Extensive experiments have highlighted the complementarity of modalities, demonstrating significant superiority over models tailored solely for images or videos.\n",
        "\n",
        "**Training Dataset:**\n",
        "- The images pretraining and tuning datasets are sourced from LLaVA.\n",
        "- The videos pretraining dataset is from Valley, while the videos tuning dataset is from Video-ChatGPT.\n",
        "\n",
        "**Getting Started with the Model:**\n",
        "- To begin using the model, users can refer to the provided code snippet, which involves importing essential libraries such as PIL, av, transformers, and huggingface_hub.\n",
        "- The code includes functions for video reading with PyAV decoder and response generation based on prompts containing video or image inputs.\n",
        "\n",
        "**Acknowledgments:**\n",
        "- The model credits LLaVA as the foundational codebase and acknowledges Video-ChatGPT for contributing evaluation code and dataset.\n",
        "\n",
        "**License and Citation:**\n",
        "- The majority of the project is released under the Apache 2.0 license for non-commercial use, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT.\n",
        "- Researchers are encouraged to cite and acknowledge the model and code if found beneficial for research purposes.\n",
        "\n",
        "For comprehensive details, including the paper and resources, please visit the GitHub repository: [Video-LLaVA GitHub](https://github.com/PKU-YuanGroup/Video-LLaVA). üöÄ‚ú®\n",
        "----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Lb2Zte8aI-"
      },
      "source": [
        "## Step 1: Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGFWdobL8aJB"
      },
      "outputs": [],
      "source": [
        "# Install the ffmpeg package for handling multimedia data such as audio and video files\n",
        "%pip install ffmpeg\n",
        "\n",
        "# Install the Pillow package for image processing tasks such as opening, manipulating, and saving image files\n",
        "%pip install pillow\n",
        "\n",
        "# Install the transformers package for natural language processing tasks using pre-trained models\n",
        "%pip install -U transformers\n",
        "%pip install -U sentencepiece\n",
        "# Install the huggingface_hub package for accessing the Hugging Face model hub for pre-trained models\n",
        "%pip install huggingface_hub\n",
        "\n",
        "# Install the av package for working with audio and video data in Python\n",
        "%pip install av"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahbj23iG8aJF"
      },
      "source": [
        "## Step 2: Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQvYe5uE8aJG"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import av\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download  # Only needed if downloading video or image from Hugging Face (specific to this notebook)\n",
        "from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc_fC0Vk8aJH"
      },
      "source": [
        "## Step 3: Initialize Start and End Indices\n",
        "‚öôÔ∏è In this step, the function sets the start and end indices based on the first and last elements of the input `indices` list. These indices determine the range of frames to be processed from the video container.\n",
        "\n",
        "### Function Summary:\n",
        "üìπ `read_video_pyav` function reads frames from a video using the PyAV library. It takes a video container and a list of indices as input.\n",
        "1. It initializes an empty list to store selected frames.\n",
        "2. It sets the start and end indices based on the input list.\n",
        "3. It iterates over the frames, selects frames within the specified range and indices, and adds them to the list.\n",
        "4. Finally, it returns a NumPy array containing the stacked RGB frames in ndarray format.\n",
        "\n",
        "üöÄ This function efficiently extracts specific frames from a video based on the provided indices, enabling targeted frame processing. Feel free to reach out if you have any questions or need further clarification! üåüüé•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FthgkNXG8aJI"
      },
      "outputs": [],
      "source": [
        "def read_video_pyav(container, indices):\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCnGpdc-8aJJ"
      },
      "source": [
        "## Step 3: Download model and processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDFQxLAK8aJK"
      },
      "outputs": [],
      "source": [
        "model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n",
        "processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMB_pzgD8aJM"
      },
      "source": [
        "## Defining prompt and video_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXAqATWx8aJM"
      },
      "outputs": [],
      "source": [
        "prompt = \"USER: <video>What is this video about? ASSISTANT:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYVOjfPf8aJN"
      },
      "outputs": [],
      "source": [
        "video_path =\"/teamspace/studios/this_studio/mixkit-person-watering-a-small-plant-by-hand-33422-medium.mp4\"\n",
        "container = av.open(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdAS38Cy8aJN"
      },
      "source": [
        "## Step 4: Processing Inputs\n",
        "üî¢ In this step, the code processes the inputs for a model by utilizing the `processor`. It combines the provided text prompt with the extracted video frames stored in the `clip` variable, converting them into tensor format and setting the `return_tensors` parameter to \"pt\".\n",
        "\n",
        "### What does this cell do?:\n",
        "üé• **Sampling Frames:** The code calculates the total number of frames in the video and generates indices to uniformly sample 8 frames from the video.\n",
        "üîç **Reading Video Frames:** It extracts the selected frames using the `read_video_pyav` function and stores them in the `clip` variable.\n",
        "üöÄ **Processing Inputs:** The code prepares inputs for a model by combining the text prompt and video frames in tensor format using the `processor`.\n",
        "\n",
        "üåü This code segment effectively samples, reads, and processes video frames along with text prompts to prepare inputs for model utilization, setting the stage for further analysis or tasks involving multimedia data.\n",
        "\n",
        "Feel free to place this markdown above the code cell in your Colab notebook for a comprehensive understanding of the code functionality! Let me know if you need any more assistance! üöÄüåü"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8RK9KzC8aJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "total_frames = container.streams.video[0].frames\n",
        "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
        "clip = read_video_pyav(container, indices)\n",
        "\n",
        "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu5G-yQE8aJO"
      },
      "source": [
        "## Step 5: Generating Output\n",
        "üîÆ In this step, the code generates output using the model by calling the `generate` method with the prepared inputs stored in the `inputs` variable. The `max_length` parameter is set to 80 for the generated output.\n",
        "\n",
        "### What does this cell do?::\n",
        "üöÄ **Generating Output:** The code utilizes the model to generate output by passing the prepared inputs to the `generate` method, which produces output based on the specified maximum length of 80 tokens.\n",
        "üìÑ **Decoding Output:** The generated output IDs are decoded using the `batch_decode` method from the `processor`, skipping special tokens and maintaining tokenization spaces for readability.\n",
        "\n",
        "üåü This code segment efficiently generates output based on the provided inputs using the model, completing the process of utilizing text prompts and video frames for model inference and analysis.\n",
        "\n",
        "Feel free to place this markdown above the code cell in your Colab notebook to provide a clear explanation of the code functionality! Let me know if you need any further assistance! üöÄüåü"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fILeYPBE8aJP"
      },
      "outputs": [],
      "source": [
        "# Generate\n",
        "generate_ids = model.generate(**inputs, max_length=80)\n",
        "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UNVuP5g8aJP"
      },
      "source": [
        "# Generate response from images and videos mix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEQ7Qn5M8aJP"
      },
      "outputs": [],
      "source": [
        "# Generate from images and videos mix\n",
        "url = r\"/teamspace/studios/this_studio/photo_2024-03-14_12-55-17.jpg\"\n",
        "image = Image.open(url)\n",
        "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "# image = Image.open(requests.get(url, stream=True).raw)\n",
        "prompt = [\n",
        "    \"USER: <image>What is in this image? ASSISTANT:\",\n",
        "    \"USER: <video>Why is this video about? ASSISTANT:\"\n",
        "]\n",
        "inputs = processor(text=prompt, images=image, videos=clip, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(**inputs, max_length=80)\n",
        "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
