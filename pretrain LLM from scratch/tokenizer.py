# -*- coding: utf-8 -*-
"""Own tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a5SHl4CWzksICv3f-4wRDT4rJGDKWiaT

# Modify old open source tokenizer

**This Line installs NLP packages: `datasets`, `transformers`, and `sentencepiece`.**
"""

!pip install datasets transformers[sentencepiece]

"""***Dataset Loading***"""

from datasets import load_dataset

"""Here i am using my own dataset but you can use any of your choice"""

dataset = load_dataset("OEvortex/Vortex-50k", split="train")

"""We can have a look at the dataset, which as **50000** texts:"""

dataset

dataset[1]

dataset[:10]

batch_size = 20000

all_instructions = [dataset[i : i + batch_size]["instruction"] for i in range(0, len(dataset), batch_size)]
all_outputs = [dataset[i : i + batch_size]["output"] for i in range(0, len(dataset), batch_size)]

def batch_iterator():
    for i in range(0, len(dataset), batch_size):
        instructions = dataset[i : i + batch_size]["instruction"]
        outputs = dataset[i : i + batch_size]["output"]
        for instruction, output in zip(instructions, outputs):
            yield instruction + " " + output

"""****Here I am using a modified GPTNeoX Tokenizer****"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("OEvortex/HelpingAI-3B")

"""****Checking if this tokenizer is fast or not.****"""

tokenizer.is_fast

"""Then we feed the training corpus (either the list of list or the iterator we defined earlier) to the train_new_from_iterator method. We also have to specify the vocabulary size we want to use:"""

new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)

"""Testing new tokenizer"""

new_tokenizer(dataset[:30]["instruction"])

"""save tokenizer"""

new_tokenizer.save_pretrained("HelpingAI")

"""**Pusinging New tokenizer to huggingface hub**"""

from huggingface_hub import notebook_login

notebook_login()

"""it is necessary that you have git lfs installed"""

!apt install git-lfs

new_tokenizer.push_to_hub("HelpingAI")