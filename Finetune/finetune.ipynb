{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8ipm8BWvVtB"
      },
      "source": [
        "# Made With üíì By - Sree ( Devs Do Code ) and by OEvortex\n",
        "\n",
        "For any questions or concerns, reach out to us via our social media handles.\n",
        "Our top choice for contact is [Telegram](https://t.me/devsdocode).\n",
        "You can also find us on other platforms listed above. We're here to help!\n",
        "\n",
        "- [YouTube Channel](https://www.youtube.com/@DevsDoCode)\n",
        "- [Telegram Group](https://t.me/devsdocode)\n",
        "- [Discord Server](https://discord.gg/ehwfVtsAts)\n",
        "- Instagram:\n",
        "  - Personal: [sree.shades_](https://www.instagram.com/sree.shades_/)\n",
        "  - Channel: [devsdocode_](https://www.instagram.com/devsdocode_/)\n",
        "\n",
        "---\n",
        "\n",
        "Dive into the world of coding with Devs Do Code - where passion meets programming!\n",
        "Make sure to hit that Subscribe button to stay tuned for exciting content!\n",
        "\n",
        "**Pro Tip:** For optimal performance and a seamless experience, we recommend using\n",
        "the default library versions demonstrated in this demo. Your coding journey just\n",
        "got even better! Happy coding!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git datasets bitsandbytes einops --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz6sXU3RCC1g"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login() # Logging into the Hugging Face Hub from a notebook environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1LgsOcsZ83_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    %pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    %pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üåü **4-Bit Quantization Explained:**\n",
        "\n",
        "4-bit quantization is a technique used to compress data, especially in machine learning. It involves reducing the number of bits used to represent a value down to just 4 bits.\n",
        "\n",
        "üßÆ **Understanding the Concept:**\n",
        "\n",
        "- **Bits and Numbers:** Computers store information using bits, which can be 0 or 1. More bits allow for a wider range of numbers to be represented with greater precision.\n",
        "- **Quantization:** Think of it like compressing an image by reducing the number of colors. Similarly, in machine learning, quantization reduces the bits used to represent values, such as weights in neural networks.\n",
        "- **4-Bit Quantization:** Specifically, this method uses only 4 bits to represent each value, significantly cutting down on memory requirements compared to traditional 16-bit or 32-bit formats.\n",
        "\n",
        "üîπ **Benefits of 4-Bit Quantization:**\n",
        "\n",
        "- **Reduced Memory Footprint:** Perfect for running large models on devices with limited resources, like language models.\n",
        "- **Faster Processing:** Smaller models may process information more quickly, leading to faster inference times.\n",
        "\n",
        "üî∏ **Challenges of 4-Bit Quantization:**\n",
        "\n",
        "- **Loss of Accuracy:** There might be a slight decrease in model accuracy due to reduced bit precision.\n",
        "- **Computational Overhead:** Implementing 4-bit quantization algorithms can introduce additional computational complexity during training.\n",
        "\n",
        "üöÄ **In Conclusion:**\n",
        "\n",
        "4-bit quantization is a powerful technique for compressing machine learning models, making them suitable for deployment on memory-constrained devices and potentially enhancing processing speed. However, striking a balance between compression and accuracy is essential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnPvDCMdar9U"
      },
      "outputs": [],
      "source": [
        "# Import the FastLanguageModel from the unsloth library\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Import the torch library for tensor computations\n",
        "import torch\n",
        "\n",
        "# Set the maximum sequence length for the language model\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Set the data type for the model, None means it will use the default data type\n",
        "dtype = None\n",
        "\n",
        "# Set the flag to load the model in 4-bit quantization to reduce memory usage\n",
        "load_in_4bit = True # set it to true as it will help you finetune model faster\n",
        "\n",
        "# List of 4-bit models available in the unsloth library\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "]\n",
        "\n",
        "# Load the pretrained model and tokenizer from the unsloth library\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\", # Specify the model name\n",
        "    max_seq_length = max_seq_length, # Specify the maximum sequence length\n",
        "    dtype = dtype, # Specify the data type\n",
        "    load_in_4bit = load_in_4bit, # Specify whether to load the model in 4-bit quantization\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNXQlSIue3FP"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e84av71we4G_"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"OEvortex/uncensored-vortex\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üåü **PEFT (Parameter-Efficient Fine-Tuning) Explained:**\n",
        "\n",
        "**Goal:** PEFT aims to streamline the training process for large language models (LLMs) by fine-tuning only a subset of parameters, making it more efficient.\n",
        "\n",
        "**Benefits:**\n",
        "- üöÄ Reduces computational cost and storage needs significantly.\n",
        "- üí° Enables training LLMs on everyday hardware.\n",
        "- üìà Achieves performance similar to fully fine-tuned models in many scenarios.\n",
        "\n",
        "**How it Works:**\n",
        "- üõ†Ô∏è Utilizes techniques like:\n",
        "    - **LoRA (Low-Rank Adaptation):** Introduces a low-rank adapter module with fewer parameters, focusing on task-specific learning while preserving the base model's parameters.\n",
        "    - **Soft Prompting:** Embeds task-specific cues strategically in the input sequence to guide the model towards desired outcomes without extensive parameter adjustments.\n",
        "\n",
        "**Applications:**\n",
        "- üåê Fine-tuning LLMs for tasks like text classification, question answering, and more.\n",
        "- üåü Democratizing LLM training by making it accessible to users with modest hardware resources.\n",
        "\n",
        "üîó **Resources:**\n",
        "- PEFT Library: [PEFT Documentation](https://huggingface.co/docs/peft/en/index)\n",
        "- GitHub Repository: [PEFT GitHub](https://github.com/huggingface/peft)\n",
        "\n",
        "---\n",
        "\n",
        "üîç **SFT (Supervised Fine-Tuning) Overview:**\n",
        "\n",
        "**Process:**\n",
        "1. Train a large LLM on a vast dataset.\n",
        "2. Fine-tune this pre-trained model on a smaller task-specific dataset to enhance its performance on that particular task.\n",
        "\n",
        "**Challenges:**\n",
        "- üíª Computationally intensive and time-consuming.\n",
        "- üìä Demands significant hardware resources.\n",
        "\n",
        "**Relation to PEFT:**\n",
        "- üîó SFT forms the basis of fine-tuning, which PEFT seeks to optimize.\n",
        "- üîÑ PEFT techniques can be integrated into an SFT framework for efficient results with reduced computational costs.\n",
        "\n",
        "üîó **Resources:**\n",
        "- Supervised Fine-tuning Trainer: [SFT Documentation](https://huggingface.co/docs/transformers/en/training)\n",
        "\n",
        "üöÄ **In Summary:**\n",
        "\n",
        "PEFT offers an efficient alternative to traditional SFT for fine-tuning LLMs, reducing training time and resource requirements while maintaining performance. This approach broadens access to LLM training and enhances efficiency across various tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFWTbMJTfHFh"
      },
      "outputs": [],
      "source": [
        "# here we are using supervised Fine-tuning\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2, # maping dataset 2 times\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2, # per device training batch size\n",
        "        gradient_accumulation_steps = 4, \n",
        "        warmup_steps = 5,\n",
        "        max_steps = 200,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOIPc4M4fK2p"
      },
      "outputs": [],
      "source": [
        "trainer.train() # starts training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaSjP_zbBuU3"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(new_model) # saving our new adapter model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For 4bit model\n",
        "  *Merging adapter model with 4bit model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This script is designed to merge a base model with an adapter model\n",
        "\n",
        "import gc  # Importing garbage collector module for memory management\n",
        "import os  # Importing operating system module for file and directory operations\n",
        "\n",
        "import torch  # Importing PyTorch library\n",
        "# from datasets import load_dataset  # Importing function to load datasets\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training  # Importing modules for PEFT training\n",
        "from transformers import (  # Importing various components from the Transformers library\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import ORPOConfig, ORPOTrainer, setup_chat_format  # Importing modules for TRL training\n",
        "\n",
        "# Model configuration\n",
        "base_model = \"m-a-p/OpenCodeInterpreter-DS-1.3B\"  # Define the base model\n",
        "new_model = \"LLama-3-8b-Uncensored\"  # Define the name of the new model to be created\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSQ0llbdB8Yg"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    offload_buffers=True\n",
        ")\n",
        "fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n",
        "\n",
        "# Merge adapter with base model\n",
        "model = PeftModel.from_pretrained(fp16_model, new_model)\n",
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr9AB9fwCAPA"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full model\n",
        " *Merging adapter model with Full model*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This script is designed to merge a base model with an adapter model\n",
        "\n",
        "import gc  # Importing garbage collector module for memory management\n",
        "import os  # Importing operating system module for file and directory operations\n",
        "\n",
        "import torch  # Importing PyTorch library\n",
        "# from datasets import load_dataset  # Importing function to load datasets\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training  # Importing modules for PEFT training\n",
        "from transformers import (  # Importing various components from the Transformers library\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import ORPOConfig, ORPOTrainer, setup_chat_format  # Importing modules for TRL training\n",
        "\n",
        "# Model\n",
        "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\" # full base model name (don't use unsloth models here)\n",
        "new_model = \"LLama-3-8b-Uncensored\" # sirif model name, it is the folder in which your adapter model is present\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    offload_buffers=True\n",
        ")\n",
        "fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n",
        "\n",
        "# Merge adapter with base model\n",
        "model = PeftModel.from_pretrained(fp16_model, new_model)\n",
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantization to GGUF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GGUF stands for GGML Universal Format. It's a specific type of file format designed to store models for a process called inference, especially when it comes to large language models (LLMs) like GPT.  In simpler terms, it's a way to save and use these models efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ü§ñ **Quantization in AI**\n",
        "\n",
        "Quantization in AI is an essential technique that involves **reducing the precision or bit-width of numerical data** in a neural network model. It's like giving the model a makeover to optimize its performance on devices with **limited computational resources**, such as mobile phones or embedded systems.\n",
        "\n",
        "During quantization, the model's **floating-point values** are transformed into **fixed-point or integer values** with a **reduced number of bits**. This clever compression technique helps to **save memory and computational power**, making the model more efficient when deployed on hardware with lower precision capabilities.\n",
        "\n",
        "However, it's worth noting that quantization comes with a trade-off between **model accuracy and efficiency**. When we reduce the precision, there's a chance of losing some valuable information, which can lead to a decline in the model's performance. To combat this, experts employ **optimization and calibration techniques** to minimize the loss and maintain an acceptable level of accuracy.\n",
        "\n",
        "Overall, quantization plays a vital role in the AI world by enabling the deployment of neural network models on **resource-constrained devices**. By finding the perfect balance between efficiency and accuracy, we can make the most of the available hardware resources while still achieving satisfactory performance levels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why is AI Quantized? ü§î\n",
        "\n",
        "AI models are often quantized to achieve various benefits, such as:\n",
        "\n",
        "1. **Model Size Reduction:** Quantization techniques help reduce the size of AI models by representing the weights and activations with lower precision data types. üìâ This is particularly useful when deploying models on resource-constrained devices with limited storage capacity. üíæ\n",
        "\n",
        "2. **Inference Speed Improvement:** Quantized models can perform computations faster due to the reduced memory bandwidth requirements and optimized hardware instructions for low-precision operations. ‚ö°Ô∏è This enables real-time or near-real-time inference, making AI applications more efficient and responsive. üöÄ\n",
        "\n",
        "3. **Energy Efficiency:** By reducing the precision of AI models, quantization reduces the computational workload, resulting in lower power consumption. üîã This is especially important for battery-powered devices or scenarios where energy efficiency is a priority. üí°\n",
        "\n",
        "4. **Deployment Flexibility:** Quantized models can be deployed on a wide range of platforms, including edge devices, embedded systems, and IoT devices. üåê The smaller model size and improved performance make it easier to integrate AI capabilities into various applications. üì±\n",
        "\n",
        "It's important to note that quantization involves a trade-off between model performance and resource efficiency. ‚öñÔ∏è While quantized models offer benefits in terms of size and speed, they may experience a slight decrease in accuracy compared to their full-precision counterparts. üîç However, advancements in quantization techniques have significantly minimized this accuracy gap, making it a valuable optimization strategy for AI models.\n",
        "\n",
        "By quantizing AI models, we can unlock their potential to run efficiently on diverse hardware and enable widespread deployment of AI applications across different domains. üåü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        " üöÄ **AI Quantization in GGUF Format**\n",
        "\n",
        "‚ÑπÔ∏è **Quantizing AI models is crucial for optimizing performance and memory usage.**\n",
        "\n",
        "If you wish to quantize your AI models in the GGUF format, head over to the following Google Colab notebook:\n",
        "[Quantization in GGUF Format Colab](https://colab.research.google.com/drive/1zmrF7Jhe_q4fNLupSWyt1bX0mqilE8sa#scrollTo=fD24jJxq7t3k)\n",
        "\n",
        "üîß **Explore the notebook to leverage the benefits of GGUF quantization for enhanced AI model efficiency!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
